5章 SVM
線形/非線形　の分類，回帰，外れ値検出　ができる強力で柔軟な機械学習モデル
中小規模のデータセットの分類に特に適してる

5.1 線形SVM分類器
マージンの太い分類　汎化性能が高くてよい
道から外れたインスタンスを増やしても，決定境界には影響しない．
道のきわにあるインスタンスによって決まる，これをサポートベクトルと呼ぶ
特徴量スケーリングをすべき

5.1.1 ソフトマージン分類
ハードマージン分類：すべてのインスタンスが道に引っかからず正しい側にいる
問題点：線形分離可能じゃないとできない　外れ値に敏感になりすぎる
汎化性能が下がるので，目標は道を太くすることと，マージン違反を減らすこと
これをソフトマージン分類
scikit-learnのSVMのハイパーパラメータC，小：道太い，マージン違反大きい
SVMが過学習しているとき，Cを小さくして正則化してみる
ロジスティック回帰分類器とは異なり，各クラスの確率を出力しない

LinearSVCクラスではなく，線形カーネルのSVCクラスを使う場合
SVC(kernel='linear', C=1)
次　LinearSVCほど速くないけどアウトオブコア学習やオンライン学習を行うとき
SGDClassfier(loss='hinge', alpha=1/(m*C))

5.2 非線形SVM分類器
線形分離不可能なデータの分離
次元を増やして線形分離可能な空間にする

5.2.1 多項式カーネル
多項式特徴量の追加は簡単で，あらゆる（SVMに限らず）アルゴリズムで機能する
が
次元が低いと複雑なデータセットを処理できず，
次元が高いと特徴量が膨大な数になってモデルが遅くなりすぎる
しかし
SVMではカーネルトリック(kernel trick)というテクがある．
"ほとんど奇跡的な数学テクニック"
実際に特徴量を追加せず多項式特徴量を追加したかのようにふるまう
高い次元の多項式特徴量にも対応でき，特徴量数の組み合わせ爆発も発生しない

5.2.2 類似性特徴量の追加
非線形問題は，個々のインスタンスがランドマークにどのくらい近いかを
測定する類似性関数の結果を特徴量とする方法でも対処できる
ガウス放射基底関数(RBF:radial basis function)
ランドマークの決め方
最も簡単：個々のインスタンスの位置にランドマークを付ける
n個の特徴量を持つm個のインスタンスによる訓練セットが
m個の特徴量を持つm個のインスタンスによる訓練セットになってしまう．
特徴量多くて大変！！

5.2.3 ガウスRBFカーネル
γ，小：決定境界が滑らか　つまり正則化ハイパーパラメータ
モデルが過学習してる　ならば　γを小さくする（Cも）
カーネルはたくさん種類があり，特定のデータ構造に特化したものもある
どれを選ぶ？
訓練セットや特徴量が多い場合，まずは線形カーネル
SVC(kernel='linear')よりもLinearSVCの方がはるかに高速
訓練セットがあんまり大きくないときは，ガウスRBFカーネルも試す
まだ時間に余力がある場合，交差検証とグリッドサーチを使ってほかのカーネルを試す．
また，特定の専門のカーネルも試す．

5.2.4 計算量
n:特徴量
m:インスタンス
LinearSVCは O(nm)
SVC O(m^2 n) < O < O(m^3 n)
まぁ遅い
でも，疎な特徴量では，特徴量数に対するスケーラビリティは良好

5.3 SVM回帰
マージンを減らしながら，道に入るインスタンスができる限り多くなるようにする．
マージン違反：道に入っていないこと（回帰では）
位置の太さ　ハイパーパラメータε
SVRクラスはSVCクラスの回帰バージョンみたい

5.4 舞台裏で行われていること
ちょっちきちいので飛ばします

5.5 演習問題
1. サポートベクトルマシンの基本的な考え方は何か．
データセットを二分するできるだけ広い道を見つける
(A)クラスとクラスの間にできる限り太い「道」を通そうとすること．
つまり，2つのクラスの決定境界の間にできる限り大きいマージンを確保すること．

2. サポートベクトルとは何か．
その道に接するインスタンス
(A)訓練後に道の中に入るインスタンス

3. SVMを使うときに入力をスケーリングするのが重要なのはなぜか
道の幅が狭くなってしまうのを防ぐため
(A)SVMが小さな特徴量を無視しがちになる．

4. SVM分類器は，インスタンスを分類するときに確信度のスコアを出力できるか．
確率はどうか．
できない．1or0
(A)できる
テストインスタンスと決定境界の距離を出力できるので，
それを確信度のスコアとして使える．
しかし推定確率に直接変換できない．
scikit-learnでSVMを作るとき，probability=Trueを設定すると，SVM訓練後にロジスティック回帰を使って確率に変換してくれる．

5. 特徴量が数百個，インスタンスが数百万個の訓練セットでモデルを訓練するとき，
SVMの主問題と双対問題どちらを使うべきか．
こっから理論やん！いやムリー(ロバート山本)
(A)
カーネル化SVMが使えるのは双対形式だけ．
そのためこの問が成り立つのは，線形SVMだけだが，答えは主問題
主問題はO(m)，双対問題はO(m^2)からO(m^3)なので使い物にならない．


