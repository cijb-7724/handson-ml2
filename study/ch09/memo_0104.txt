9.1.5 半教師あり学習の一部としてのクラスタリング
・ラベル伝播
ラベルなしのインスタンスについてクラスタリングをして，代表インスタンスに対して手作業で
ラベリングを行う．同じクラスタに含まれるインスタンスのラベルを代表インスタンスのラベルとする．
これだけではだめ，
クラスタ境界に近いインスタンスには，間違ったラベルがつけられる可能性があるから．
重心に近い20%のインスタンスだけにラベルイングをする．→さらに性能が上がる．

・能動学習
すべてのラベル付きインスタンスでモデルを学習し，ラベルなしインスタンスを予測したとき，
予測確率が最も低いインスタンスのラベルを専門家に尋ねるなど．

9.1.6 DBSCAN
ε近傍内にmin-sanple個のインスタンスがあったらそのインスタンスは「コアインスタンス」
コアインスタンスでなく，近くにコアインスタンスのないインスタンスは異常値のみなされる．
クラスタによって密度が大きく異なる場合は，適切に機能するとは限らない．

9.1.7 その他のクラスタリングアルゴリズム
・凝縮クラスタリング クラスタの二分木
・BIRCH 大規模なデータセットを使う目的で設計．高速でK平均法と同様の結果．
・平均値シフト法 DBSCANに似てる
・アフィニティ伝播法 投票システムを使う．個々のインスタンスは自分の代表として類似インスタンスを投票し，収束する．
・スペクトラルクラスタリング インスタンス間の類似度行列を入力として，次元削減をする．

9.2 混合ガウスモデル
混合ガウスモデルGMM(gaussian mixture model)
EM法：ｋ平均法の一般形
混合ガウスモデルは生成的なモデル：モデルから新しいインスタンスをサンプリングすることができる．
データが楕円体に分布しているとうまくいくっぽい．yes
クラスタ数が多く，インスタンスが少ないとEMは最適解に収束するのが困難．パラーメータを制限する．

9.2.1 混合ガウスモデルを使った異常検知
異常検知（外れ値検知）→詐欺検知，製造ラインの不良品検知，前処理としての外れ値除去などの応用分野がある．
新規検知：アルゴリズムがクリーンなデータセットで訓練されることが前提となっている点で上と異なる．

9.2.2 クラスタ数の選択
ベイズ情報量基準(BIC)，赤池情報量基準(AIC)
ともに，学習するパラメータが多いモデルにペナルティを与え，データによく適合するモデルに報酬を与える．

9.2.3 混合ベイズガウスモデル
最適なクラスタ数を機械的に探す方法．ある程度タスクに知識があることが前提で，クラスタ数を決めると，
不要なクラスタを削除してくれる．

9.2.4 以上/新規検知のためのその他のアルゴリズム
任意の形のクラスタを扱える，クラスタリングアルゴリズムたち．
・PCA
・Fast-MCD
・アイソレーション
・LOF
・1クラスSVM

9.3 演習問題
1. あなたならクラスタリングをどのように定義するか．また，クラスタリングアルゴリズムをいくつか挙げなさい．
ラベルのないデータ群を適切にグループ分けすること．
k平均法・混合ガウスモデル，


