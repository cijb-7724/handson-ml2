9.1.1.1 K近傍法のアルゴリズム
無作為にk個のインスタンスを選び重心とする．
インスタンスにラベルを付け，重心を更新．重心が動かなくなるまで繰り返す．
有限回で収束することが保証されている．
計算量：インスタンス数m，クラスタ数k，次元数nいずれに対しても線形
（データにクラスタリング構造がある場合だけ）
最悪の場合計算量は指数関数的に上がっていく．
収束はするが正しいとは限らない．（局所最適解に収束することがある）
このリスクを緩和するために重心の初期化をどのようにすればよいのか？次へ

9.1.1.2 重心の初期化方法
おおよその重心位置がわかってたら，initハイパーパラメータに重心リスト追加する
複数の無作為な初期値を使ってアルゴリズムを何度も実行しおおよその初期値を見つける．
初期値を設定する回数はn_initハイパーパラメータで制御できる
解が最良であるとは？慣性(inertia)：最近傍重心の平均二乗距離
score = -inertia
K-means++ 最適解見つけやすい初期値
1.データセットから一様無作為に重心c(1)を選ぶ
2.インスタンスx(i)と既に選択された最近傍重心との距離をD(x(i))として，
確率pで選ばれたインスタンスx(i)を新しい重心c(i)と線形にどのような関係になっているかを測定する．
この確率分布により，既に選択された重心から遠いところにあるインスタンスうが新たな重心として選ばれやすくなる．
3.重心をk個選ぶまでステップ2を繰り返す．
sklearnのKMeansクラスはデフォでこの初期値方法．

9.1.1.3 Accelerated K-MeansとミニバッチK平均法
不要な距離計算の多くを省略してアルゴリズムのスピードを高速化
三角不等式や重心間の距離の上限下限の管理など
sklearnのKMeansクラスはデフォでこれ使ってる．
ミニバッチK平均法は高速だけど慣性の品質は一般にほんのわずかに劣る．（特にクラスタ数がおおいと）

9.1.1.4 最適なクラスタ数の見つけ方
慣性はクラスタ数が増えると小さくなるので指標に使えない．
雑な方法：kに対する慣性をプロットした曲線の「ひじ」になるkを選択する．
シルエット係数の平均，シルエットスコアを用いた方が正確．

9.1.2 K近傍法の限界
非最適解に引っかかってしまうので完璧ではない．
K近傍法を実行する前に入力特徴量のスケーリングをするのが大切．

9.1.3 画像セグメンテーションとしてのクラスタリング
画像セグメンテーションは，画像を複数のセグメントに分割するタスク．
セマンティックセグメンテーションでは，同じ物体に属するすべてのピクセルを同じセグメントに割り振る．（歩行者は全部それ）
インスタンスセグメンテーションは，同じ個体に属するすべてのピクセルを同じセグメントに割り振る．（別の歩行者は別のセグメント）
カラーセグメンテーション：似ている色を同じにしちゃう．
