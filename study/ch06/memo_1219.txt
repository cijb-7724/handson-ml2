6章 決定木
分類にも回帰にも使えて，多出力タスクもできる柔軟なモデル．

6.1 決定木の訓練と可視化
プログラムを実行した．

6.2 決定木による予測
決定木を辿る方法
特徴量スケーリングやセンタリングが必要ない場合が多い．
純粋：ノードの条件に当てはまるすべてのインスタンスが同じクラスに属する
不純度の指標：ジニ係数
scikit-learnは二分木しか作らないCARTアルゴリズムを使っている

ホワイトボックスモデル：決定木はモデルの説明も判断方法も説明が簡単
ブラックボックスモデル：予測したモデルの根拠を示すのが難しい

6.3 クラスの確率の推計
インスタンスがクラスkに属する確率
決定木軒構造を辿って該当インスタンスの葉ノードを見つける
そのノード内にあるクラスkの訓練インスタンスの割合

6.4 CART訓練アルゴリズム
Classification and Regression Treeの略
ある特徴量kと閾値t_kを使って，訓練セットを2つのサブセットに分割する．
サブセットに対しても同様の操作を行い再帰的に木を構成する．
これはGreedyなアルゴリズム
最適な木を見つけるという問題はNP完全O(exp(m))

6.5 計算量
インスタンス数n, 特徴量mでO(mnlogm)

6.6 ジニ不純度かエントロピーか
デフォルトでジニ不純度
ハイパーパラメータでエントロピーにもできる
ジニ不純度の方がわずかに高速
エントロピーはわずかに平衡の取れた木を作る

6.7 正則化ハイパーパラメータ
- ノンパラメトリックモデル（決定木）
-- パラメータをたくさん持っている
-- データ構造がデータに密接に適合できる
-- 過学習しやすい
- パラメトリックモデル
-- 決められた数のパラメータがある
-- 自由度が制限され，過学習のリスクが低くなる（過小適合しやすい）
max_depthをデフォはNoneだが，設定する
min_samples_leaf：葉ノードが持たなければいけないサンプル数の下限
min_weight_fraction_leaf：上と同じ，重み付きインスタンスに対する割合
maxもそれぞれある．

6.8 回帰
正則化しないとひどく過学習してしまう
max_depthをいじる

6.9 不安定性
訓練セットの回転に対して不安定である．
PCA主成分分析である程度軽減できる







