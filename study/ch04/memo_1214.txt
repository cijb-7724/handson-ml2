4.6.4 ソフトマックス回帰
ソフトマックス回帰　別名多項ロジスティック回帰
実数列を確率表現（和が１，全部正数）に変換する．
多クラス分類だが多出力ではない．
損失関数は交差エントロピーと相性が良い

4.7 演習問題
1. 数百万個もの特徴量を持つ訓練セットがあるときに使える線形回帰訓練アルゴリズムは何か．
ミニバッチ勾配降下法，確率的勾配降下法

2. 訓練セットの特徴量のスケールがまちまちだとする．これによって悪影響を受けるアルゴリズムは何で，
どのような影響があるか．その問題にはどのように対処すればよいか．
勾配降下法
収束が遅くなる．
すべてのデータを用いず，バッチ勾配降下法または，確率的勾配降下法にする．
(A)スケーリングする

3. ロジスティック回帰モデルを訓練しているときに，勾配降下法が局所的な最小値から抜け出せなくなることはあるか．
凸関数だからない．

4. 十分な実行時間を与えれば，すべての勾配降下法アルゴリズムは同じモデルに帰着するか．
学習スケジュールが適切ならYes
そうでないとき確率的勾配降下法はNo
(A)最適化モデルが凸関数かつ学習率が大きすぎなければ，GDは収束．
学習率を次第に小さくすれば，確率的GD，ミニバッチGDも収束．

5. バッチ勾配降下法を使っていて，エポックごとに検証誤差をプロットしているものとする．
検証誤差が絶えず大きくなっていることに気づいた場合，何が起きていると考えられるか．
この問題はどのように修正すればよいか．
学習不足
．．．
(A)学習率が高すぎてアルゴリズムが発散している可能性がある．
訓練誤差も上がってるなら学習率関係．
訓練誤差が上がらないなら過学習

6. 検証誤差が上がりだしたときにミニバッチ勾配降下法をすぐに中止するのはよいことか．
よいこと．
(A)よくない．
ミニバッチはランダムに選ばれたインスタンスで計算するため，
訓練するたびに誤差が下がる保証がない．

7. 本書で取り上げた勾配降下法アルゴリズムの中で，最適な解の近辺にもっとも速く到達するのはどれ．
それは実際に収束するか．ほかの勾配降下法はどうすれば収束するか．
(A)
確率的勾配降下法，場合によってはミニバッチ勾配降下法
収束しない．学習率を次第に下げると収束する．

8. 多項式回帰を使ているものとする．学習曲線をプロットしたところ，訓練誤差と検証誤差の間に大きな誤差があった．
何が起きているか．：過学習
この問題を解決するための方法を3つ挙げなさい．
・多項式に用いる次元を下げる．
(A)
・モデルを正則化する
・訓練セットのサイズを大きくする．

9. Ridge回帰を使っていて，訓練誤差と検証誤差がほとんど同じだが，非常に高いことに気づいた．
バイアスと分散どちらが高いとそうなるか．：バイアス　過小適合してしまってるから
正則化ハイパーパラメータのaは上げるべきか下げるべきか．：表現力を高めるために，下げるべき

10. 以下を説明しなさい．
・線形回帰（正則化項なし）ではなくRidge回帰を使うべき理由
重みなどのパラメータが大きいと，過学習を起こす恐れがあるので，パラメータの絶対値が小さくなるようにするため．
(A)正則化されてるモデルの方が性能が高くなるので一般には使うべき

・Ridge回帰ではなくLasso回帰を使うべき理由
(A)Lasso回帰は小さな重みを0にする．重要な重み以外は0になる疎なモデルを作りやすい．
意味ある特徴量がわずかだと思われるときはLassoが良い

11. 写真を屋外/屋内，日中/夜間に分類したいものとする．
２つのロジスティック回帰分類器を作るべきか，それとも1つのソフトマックス回帰分類器を作るべきか
ロジスティック回帰分類器を作るべき
(A)排他的でないとソフトマックスは使えないからそう



