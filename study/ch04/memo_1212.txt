4.1.2．計算量
n^2の行列の逆行列の計算がO(n^3)だが，scikit-learnのSVD方式ならO(n^2)．
特徴量が非常に多い場合，訓練インスタンスが多すぎて目盛りに収まらないとき正規方程式を使う．

4.2 勾配降下法
損失関数を最小にするためにパラメータを繰り返し操作する．
局所解に陥る可能性がある．
線形回帰のMSE損失関数は凸関数なので，局所解にはまらず，最小値に収束する．
勾配降下法を使うときは，すべての特徴量のスケールをそろえないと収束までの時間が長くなる．

4.2.1 バッチ勾配降下法
全データを使う勾配法
数十万個も特徴量があると，勾配法の方が正規方程式よりもはやい．

4.2.2 確率的勾配降下法
すべてのインスタンスではなく，確率的に1つのインスタンスを選び勾配法を行う．速い．

4.2.3 ミニバッチ勾配降下法
mini-batchで勾配法
GPUを使えるなら，行列演算のハードウェアによる最適化でパフォーマンスが上がる．
学習スケジュール：学習をしていく過程で学習率を変化させること
適切に学習スケジュールを設定することで，確率的でもミニバッチでも最小値へ収束する．

4.3 多項式回帰
多項式回帰：各特徴量の累乗を新特徴量として追加→拡張特徴量セットで線形モデルを訓練

4.4 学習曲線
高次にすると過学習を起こす．
何次関数でモデルを訓練するとよいのか，評価指標：学習曲線
訓練セットサイズに対するRMSE
過小適合の学習曲線：訓練セット，検証セットの性能が一定に近づき，全体として誤差が大きい，
過学習の学習曲線：訓練セットと検証セットの収束後の性能が離れている．

4.5 線形モデルの正則化
モデルの正則化（＝制約の強化）は過学習を緩和する．

4.5.1 Ridge回帰
損失関数に重みの二乗和を加算し，パラメータの絶対値が小さくなるようにする．
訓練中のみ追加することに注意
4.5.2 Lasso回帰
損失関数に加えるのは重みの絶対値の和．
重要度の低いも特徴量を自動的に選択し，疎なモデルを出力する．

4.5.3 Elastic Net
RidgeとRassoの中間．
r=0でRidge, r=1でLasso

4.5.4 早期打ち切り
検証セットの損失関数が最小値になったところでやめちゃう．
最小値になったかわかりづらいので，ずっと暫定的な最小値以上の値だったら，その時点の最小値をとったパラメータを答えとする．

4.6 ロジスティック回帰
回帰だけど分類に使える．

4.6.1 確率の推計
シグモイド関数使って実数を確率に変換する．

4.6.2 訓練と損失関数
iris dataset


4.6.3 決定境界
4.6.4 ソフトマックス回帰

