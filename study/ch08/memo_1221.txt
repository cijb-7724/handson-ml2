8章 次元削減
機械学習では訓練インスタンスごとに，数百万もの特徴量を相手にする．
解を見つけるのが困難：次元の呪い
次元削減は訓練時間だけでなく，システムの性能もちょっと下がる．
PCA，カーネルPCA，LLEという次元削減を扱う．

8.1 次元の呪い
訓練セットの次元が高いほど，過学習のリスクが高くなる

8.2 次元削減のための主要なアプローチ
射影と多様体学習

8.2.1 射影
データが疎なのが問題なので高次元のデータを低次元に射影し，データを密にする
部分空間がねじれていると1個低い次元に射影できない．
ロールケーキを展開した平面に射影したい！

8.2.2 多様体学習
スイスロールは2次元多様体の例
2次元多様体とは高次元空間で平面を回転させたり，ねじったりできる2次元図形
d次元多様体はn次元空間の一部で(d<n)，局所的にd次元超平面に似ているもの
モデルを訓練する前に次元を削減すると，訓練スピードは間違いなく速くなるが必ずしもより良い解にたどり着くとは限らない．
次元削減の効果はデータセット次第！図8-6なるほど！

8.3 PCA
主成分分析：principal component analysis
データに最も接近する超平面を見つけ，そこにデータを射影する．

8.3.1 分散の維持
分散が最大になる超平面を見つけないと情報が失われちゃう

8.3.2 主成分
PCAは分散が最大になる軸を見つけ出す，第2軸はそれに直交するように決まる．
i番目の軸をi番目の主成分という
特異値分解で主成分を見つけられる

8.3.3 低次のd次元への射影
超平面に射影する変換行列を特徴量ベクトルに掛け算する．

8.3.4 scikit-learnの使い方
8.3.5 因子寄与率
個々の主成分の因子寄与率も重要な情報．
個々の主成分に沿ったデータセットの分散の分散全体に対する割合を返す．

8.3.6 適切な次数の選択
次数をいくつまで減らせばいいのか？
因子寄与率の合計が95%になるまで　など
可視化のために3次まで　など
次数の関数ととして因子寄与率をプロットする．95%までいかなくても，いい数値が見つかるかも．

8.3.7 圧縮のためのPCA
PCA射影の逆変換を行えば，次元削減されたデータセットを784次元に再構築することもできる．
オリジナルデータと再構築されたデータの平均二乗距離を再構築誤差という

8.3.8 ランダム化PCA
svd_solverを"randomized"にするとd個の主成分の概数を高速に見つけ出して売れるRandomized PCAという確率的アルゴリズムがある．
完全な特異値分解をするとO(mn^2) + O(n^3)だが
ランダム化PCAではO(md^2) + O(d^3)になる．
d<nでかなり効率的

8.3.9 逐次学習型PCA
今までのは訓練セット全体がメモリに収まってないといけなかった．
ミニバッチに分割し，1度に1つずつミニバッチを渡して逐次学習型PCAというアルゴリズムが開発されている．
PCAをオンライン実行したいときに役立つ．

8.4 カーネルPCA
カーネルトリックを用いて非線形射影をつくれる．
kPCAは射影後にもインスタンスのクラスタをうまく保存できることが多い．
曲がりくねった多様体に近接するデータセットの展開にも使える．

8.4.1 カーネルの選択とハイパーパラメータの調整
kPCAは教師なし学習
教師あり学習タスクの前段階いなっていることが多いから，
カーンセルの選択やハイパラは，性能が最大になるようにグリッドサーチをして探せばよい．
線形PCAほど再構築は簡単ではないがプレイメージを使えばできる．

8.5 LLE
LLE(Locally linear embedding)局所線形埋め込み法も非線形次元削減のテクニック

8.6 その他の次元削減テクニック

8.7 演習問題




