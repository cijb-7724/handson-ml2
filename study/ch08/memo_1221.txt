8章 次元削減
機械学習では訓練インスタンスごとに，数百万もの特徴量を相手にする．
解を見つけるのが困難：次元の呪い
次元削減は訓練時間だけでなく，システムの性能もちょっと下がる．
PCA，カーネルPCA，LLEという次元削減を扱う．

8.1 次元の呪い
訓練セットの次元が高いほど，過学習のリスクが高くなる

8.2 次元削減のための主要なアプローチ
射影と多様体学習

8.2.1 射影
データが疎なのが問題なので高次元のデータを低次元に射影し，データを密にする
部分空間がねじれていると1個低い次元に射影できない．
ロールケーキを展開した平面に射影したい！

8.2.2 多様体学習
スイスロールは2次元多様体の例
2次元多様体とは高次元空間で平面を回転させたり，ねじったりできる2次元図形
d次元多様体はn次元空間の一部で(d<n)，局所的にd次元超平面に似ているもの
モデルを訓練する前に次元を削減すると，訓練スピードは間違いなく速くなるが必ずしもより良い解にたどり着くとは限らない．
次元削減の効果はデータセット次第！図8-6なるほど！

8.3 PCA
主成分分析：principal component analysis
データに最も接近する超平面を見つけ，そこにデータを射影する．

8.3.1 分散の維持
分散が最大になる超平面を見つけないと情報が失われちゃう

8.3.2 主成分
PCAは分散が最大になる軸を見つけ出す，第2軸はそれに直交するように決まる．
i番目の軸をi番目の主成分という
特異値分解で主成分を見つけられる

8.3.3 低次のd次元への射影
超平面に射影する変換行列を特徴量ベクトルに掛け算する．

8.3.4 scikit-learnの使い方
8.3.5 因子寄与率
個々の主成分の因子寄与率も重要な情報．
個々の主成分に沿ったデータセットの分散の分散全体に対する割合を返す．

8.3.6 適切な次数の選択
次数をいくつまで減らせばいいのか？
因子寄与率の合計が95%になるまで　など
可視化のために3次まで　など
次数の関数ととして因子寄与率をプロットする．95%までいかなくても，いい数値が見つかるかも．

8.3.7 圧縮のためのPCA
PCA射影の逆変換を行えば，次元削減されたデータセットを784次元に再構築することもできる．
オリジナルデータと再構築されたデータの平均二乗距離を再構築誤差という

8.3.8 ランダム化PCA
svd_solverを"randomized"にするとd個の主成分の概数を高速に見つけ出して売れるRandomized PCAという確率的アルゴリズムがある．
完全な特異値分解をするとO(mn^2) + O(n^3)だが
ランダム化PCAではO(md^2) + O(d^3)になる．
d<nでかなり効率的

8.3.9 逐次学習型PCA
今までのは訓練セット全体がメモリに収まってないといけなかった．
ミニバッチに分割し，1度に1つずつミニバッチを渡して逐次学習型PCAというアルゴリズムが開発されている．
PCAをオンライン実行したいときに役立つ．

8.4 カーネルPCA
カーネルトリックを用いて非線形射影をつくれる．
kPCAは射影後にもインスタンスのクラスタをうまく保存できることが多い．
曲がりくねった多様体に近接するデータセットの展開にも使える．

8.4.1 カーネルの選択とハイパーパラメータの調整
kPCAは教師なし学習
教師あり学習タスクの前段階いなっていることが多いから，
カーンセルの選択やハイパラは，性能が最大になるようにグリッドサーチをして探せばよい．
線形PCAほど再構築は簡単ではないがプレイメージを使えばできる．

8.5 LLE
LLE(Locally linear embedding)局所線形埋め込み法も非線形次元削減のテクニック
射影に依存しない多様体学習テクニック．
個々の訓練インスタンスが最近傍インスタンス(c.n.)と線形にどのような関係になっているかを測定する．
その局所的な関係から最もよく保存される訓練セットの低次元表現を探す．
ノイズの少ない曲がりくねった多様体の展開で力を発揮する．
LLEの計算量は，k個の最近傍インスタンスを見つけるためにO(m log(m) n log(k))
重みを最適化するためにO(mnk^3)
低次元表現の構築のためにO(dm^2)である．
m^2のせいで大規模なデータセットへのスケーラビリティは低い

8.6 その他の次元削減テクニック
・ランダム射影：ランダム線形射影を使って低次元空間にデータを射影する．初期次元の影響を受けない．
・多次元尺度法MDS(multidimensional scaling)：インスタンス間の距離を維持しようとしながら次元削減を行う．
・Isomap
・t-SNE
・線形判別分析

8.7 演習問題
1. データセットを次元削減する主要な理由は何か．次元削減の主要な欠点は何か．
>>特徴量が多すぎて学習に時間がかかるの改善．
>>情報量が100%維持されないので，性能が下がる可能性がある．
A
主な理由
高速化・性能向上
データの可視化
圧縮スペース節約
主な欠点
重要な情報が失われ性能が下がる可能性
CPUに負荷をかけることがある．
機械学習パイプラインの複雑化
変換後の特徴量が解釈しにくくなる

2. 次元の呪いとはなにか．
A
低次元では存在しない多くの問題が高次元空間では顕在化すること．
機械学習で最も顕著なもの
無作為に抽出した高次元ベクトルが一般に非常に疎で，過学習のリスクが上がるだけでなく，
訓練データが相当豊富になければデータの中のパターンを見つけにくくなること．

3. データセットを次元削減した後で，次元を元に戻すことはできるか．
>>可能
A>>この章で取り上げたものだけでは無理
できるならそれはどのようにしてするのか．
>>逆射影をかける
できないならなぜか．
A>>一部の情報が失われるから．非常に近い逆変換は存在する．

4. PCAは，高次元非線形データセットの次元削減に使えるか．
>>可能だが有効ではない．
A 少なくとも不要な次元を取り除けるので，まったく非線形なものを含め大幅にできる．
しかし，不要な次元がない（スイスロール）なときは失う情報が多すぎる．

5. 因子寄与率を95%に設定して1000次元のでーあつぇっとにPCAを適用する場合，
得られるデータセットの次元はどの程度になるか．
>>データセットによるから何とも言えないが，例では100次元程度
A 正解

6. 通常のPCA，逐次学習型PCA，ランダム化PCA，カーネルPCAはどのように使い分けるべきか．
>>通常のPCA：データの分散がある軸で大きいとき
A>>デフォルト，データセットがメモリい収まらないと動作しない．
>>逐次学習型PCA：訓練セットがアウトオフコアのとき
A>>大規模なデータセットでも使えるがPCAよりも遅い
>>ランダム化PCA：
A>>データセットがメモリに収まり，次元を大幅に削減したいとき，通常のPCAよりも大幅に高速になる．
>>カーネルPCA：非線形な射影をしたいとき
A 正解

7. データセットに対する次元削減アルゴリズムを続けて使うことに意味はあるか．
ない．
A 情報をあまり失わずに次元を削減できれば性能が高いといってよい．

8. 2つの異なる次元削減アルゴリズムを続けて使うことに意味はあるか．
ない．
A しっかりとした意味がある．
たとえば，PCAで不要な次元を手っ取り早く大量に取り除いてから，LLEなどの時間のかかる次元削減アルゴリズムを使うことはよくある．
この2ステップアプローチはLLE単独の場合にほぼ匹敵する性能を引き出せるが，
処理時間は数分の1になる．

